# -*- coding: utf-8 -*-
"""
å°è‚¡è‡ªå‹•æƒæç­–ç•¥æ©Ÿå™¨äºº (Scanner Bot) - V53 De-Duplicate Logic

ã€V53 ä¿®æ­£é‡é»ï¼šé‡è¤‡åå–®æ¸…æ´—ã€‘
1. æ–°å¢ remove_duplicates_keep_earliest():
   - ç¢ºä¿ History ä¸­åŒä¸€æ”¯è‚¡ç¥¨åªæœƒå‡ºç¾ä¸€æ¬¡ã€‚
   - ä¿ç•™ã€Œæœ€æ—©ã€çš„ç´€éŒ„ (Entry Date)ï¼Œè®“ ROI ä»¥æœ€åˆé€²å ´åƒ¹è¨ˆç®—ã€‚
2. æƒæçµæœéæ¿¾:
   - ä»Šæ—¥æƒæåˆ°çš„æ–°è‚¡ï¼Œè‹¥å·²å­˜åœ¨æ–¼ History (ä¸ç®¡å“ªä¸€å¤©)ï¼Œç›´æ¥å¿½ç•¥ï¼Œä¸é‡è¤‡å»ºæª”ã€‚

ã€V52 åŒ…å«åŠŸèƒ½ã€‘
1. å¼·åŒ– update_history_roi: å¢åŠ è©³ç´° Logï¼Œè™•ç† yfinance å–®æª”/å¤šæª”è³‡æ–™çµæ§‹å·®ç•°ã€‚
2. éŒ¯èª¤è™•ç†: ç•¶ yfinance ä¸‹è¼‰å¤±æ•—æ™‚ï¼Œæœƒæ˜ç¢ºå°å‡ºéŒ¯èª¤åŸå› ã€‚
3. è³‡æ–™å›è£œ: é‡å° NaN è³‡æ–™å¢åŠ  ffill() å›è£œã€‚

ã€ç­–ç•¥ Aï¼šæ‹‰å›ä½ˆå±€ã€‘
   1. é•·ç·šä¿è­·ï¼šæ”¶ç›¤ > MA240, MA120, MA60ã€‚
   2. å¤šé ­æ’åˆ—ï¼šMA10 > MA20 > MA60ã€‚
   3. ä½éšå®‰å…¨ï¼šä¹–é›¢ç‡ < 25%ã€‚
   4. å‡ç·šç³¾çµï¼šå·®ç•° < 8%ã€‚
   5. é‡ç¸®æ•´ç†ï¼šæˆäº¤é‡ < 5æ—¥å‡é‡ã€‚
   6. æ”¯æ’ç¢ºèªï¼šæ”¶ç›¤ > MA10ã€‚
   7. åº•éƒ¨æ‰“æ¨ï¼š|ä»Šæ—¥æœ€ä½ - æ˜¨æ—¥æœ€ä½| < 1%ã€‚
   8. æµå‹•æ€§ï¼š5æ—¥å‡é‡ > 500å¼µã€‚

ã€ç­–ç•¥ Bï¼šVCP æŠ€è¡“é¢ (Strict VCP)ã€‘
  1. ç¡¬æŒ‡æ¨™éæ¿¾ï¼šè‚¡åƒ¹ > MA240 & > MA60 & æˆäº¤é‡ > 500å¼µã€‚
  2. åƒ¹æ ¼ä½éšï¼šé è¿‘ 52 é€±æ–°é«˜ã€‚
  3. æ³¢å‹•æ”¶ç¸®ï¼šå¸ƒæ—å¸¶å¯¬åº¦ < 15%ã€‚
  4. é‡èƒ½éæ¸›ï¼š5æ—¥å‡é‡ < 20æ—¥å‡é‡ã€‚
  5. å›æª”æ”¶ç¸®ï¼šr1(60æ—¥) > r2(20æ—¥) > r3(10æ—¥)ã€‚
"""

import yfinance as yf
import pandas as pd
import twstock
import json
import os
import math
from datetime import datetime, time as dt_time, timedelta
import pytz
import time

# ==========================================
# 1. è³‡æ–™åº«ç®¡ç†
# ==========================================
DB_INDUSTRY = 'cmoney_industry_cache.json'
DB_HISTORY = 'history.json'
DATA_JSON = 'data.json'

def load_json(filename):
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_json(filename, data):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ==========================================
# 2. ç”¢æ¥­åˆ†é¡è§£æé‚è¼¯
# ==========================================
def get_stock_group(code, db_data):
    group = "å…¶ä»–"
    if code in db_data:
        raw_data = db_data[code]
        if isinstance(raw_data, dict):
            if 'sub' in raw_data and raw_data['sub']: group = raw_data['sub']
            elif 'main' in raw_data and raw_data['main']: group = raw_data['main']
            elif 'industry' in raw_data: group = raw_data['industry']
        elif isinstance(raw_data, str):
            group = raw_data
    elif code in twstock.codes:
        if code in twstock.codes and twstock.codes[code].group:
            group = twstock.codes[code].group.replace("å·¥æ¥­", "").replace("æ¥­", "")
    
    if not isinstance(group, str): group = str(group)
    return group

def get_all_tickers():
    twse = twstock.twse
    tpex = twstock.tpex
    ticker_list = []
    for code in twse:
        if len(code) == 4: ticker_list.append(f"{code}.TW")
    for code in tpex:
        if len(code) == 4: ticker_list.append(f"{code}.TWO")
    return ticker_list

# ==========================================
# 3. [æ–°å¢] æ­·å²åå–®æ¸…æ´—é‚è¼¯
# ==========================================
def remove_duplicates_keep_earliest(history_db):
    """
    æ¸…æ´— History DB:
    1. å°‡æ‰€æœ‰æ—¥æœŸçš„è‚¡ç¥¨æ”¤å¹³ã€‚
    2. æŒ‰æ—¥æœŸç”±èˆŠåˆ°æ–°æ’åºã€‚
    3. ä¿ç•™æ¯æ”¯è‚¡ç¥¨ã€Œç¬¬ä¸€æ¬¡ã€å‡ºç¾çš„ç´€éŒ„ï¼Œåˆªé™¤å¾ŒçºŒæ—¥æœŸçš„é‡è¤‡é …ã€‚
    4. é‡çµ„å› history_db æ ¼å¼ã€‚
    """
    if not history_db:
        return {}, set()
        
    print("ğŸ§¹ æ­£åœ¨åŸ·è¡Œæ­·å²åå–®å»é‡ (ä¿ç•™æœ€æ—©é€²å ´é»)...")
    
    # 1. å°‡è³‡æ–™æ”¤å¹³ç‚º (DateObject, DateStr, StockData) çš„åˆ—è¡¨
    flat_list = []
    for date_str, stocks in history_db.items():
        try:
            dt = datetime.strptime(date_str, "%Y/%m/%d")
            for stock in stocks:
                flat_list.append({
                    'dt': dt,
                    'date_str': date_str,
                    'stock': stock
                })
        except: continue

    # 2. æŒ‰æ—¥æœŸæ’åº (èˆŠ -> æ–°)
    flat_list.sort(key=lambda x: x['dt'])

    # 3. éæ¿¾é‡è¤‡ ID
    seen_ids = set()
    cleaned_map = {} # key: date_str, value: list of stocks

    duplicates_removed = 0
    
    for item in flat_list:
        stock_id = item['stock']['id']
        date_str = item['date_str']
        
        if stock_id not in seen_ids:
            seen_ids.add(stock_id)
            # åŠ å…¥æ–°åå–®
            if date_str not in cleaned_map:
                cleaned_map[date_str] = []
            cleaned_map[date_str].append(item['stock'])
        else:
            duplicates_removed += 1
    
    print(f"ğŸ§¹ æ¸…æ´—å®Œæˆ: ç§»é™¤äº† {duplicates_removed} ç­†é‡è¤‡è³‡æ–™ã€‚")
    return cleaned_map, seen_ids

# ==========================================
# 4. ç­–ç•¥é‚è¼¯ (ä¿ç•™åŸå§‹è©³ç´°é‚è¼¯)
# ==========================================

def check_strategy_original(df):
    if len(df) < 250: return False, None
    close = df['Close']
    volume = df['Volume']
    low = df['Low']
    
    ma5 = close.rolling(5).mean()
    ma10 = close.rolling(10).mean()
    ma20 = close.rolling(20).mean()
    ma60 = close.rolling(60).mean()
    ma120 = close.rolling(120).mean()
    ma240 = close.rolling(240).mean()
    vol_ma5 = volume.rolling(5).mean()
    
    curr_c = float(close.iloc[-1])
    curr_v = float(volume.iloc[-1])
    curr_l = float(low.iloc[-1])
    
    curr_ma5 = float(ma5.iloc[-1])
    curr_ma10 = float(ma10.iloc[-1])
    curr_ma20 = float(ma20.iloc[-1])
    curr_ma60 = float(ma60.iloc[-1])
    curr_ma120 = float(ma120.iloc[-1]) 
    curr_ma240 = float(ma240.iloc[-1])
    curr_vol_ma5 = float(vol_ma5.iloc[-1])
    
    prev_l = float(low.iloc[-2])

    # === å¼·åˆ¶æª¢æŸ¥ MA240 (åš´æ ¼éæ¿¾) ===
    if math.isnan(curr_ma240): return False, None # è³‡æ–™ä¸è¶³ï¼Œå‰”é™¤
    if curr_c < curr_ma240: return False, None    # è·Œç ´å¹´ç·šï¼Œå‰”é™¤

    # éæ¿¾ï¼šæˆäº¤é‡é–€æª»
    if curr_vol_ma5 < 500000: return False, None 

    # 1. é•·ç·šä¿è­· (åŒ…å« MA60 èˆ‡ MA120 æª¢æŸ¥)
    if curr_c <= curr_ma120 or curr_c <= curr_ma60: return False, None
    
    # 2. å¤šé ­æ’åˆ—
    if not ((curr_ma10 > curr_ma20) and (curr_ma20 > curr_ma60)): return False, None
    
    # 3. ä½éšæ§åˆ¶
    bias_ma60 = (curr_c - curr_ma60) / curr_ma60
    if bias_ma60 >= 0.25: return False, None
    
    # 4. å‡ç·šç³¾çµ
    mas = [curr_ma5, curr_ma10, curr_ma20]
    ma_divergence = (max(mas) - min(mas)) / min(mas)
    if ma_divergence >= 0.08: return False, None
    
    # 5. é‡ç¸®æ•´ç†
    if curr_v >= curr_vol_ma5: return False, None
    # 6. æ”¯æ’ç¢ºèª
    if curr_c <= curr_ma10: return False, None
    
    # 7. åº•éƒ¨æ‰“æ¨
    if prev_l > 0:
        low_diff_pct = abs(curr_l - prev_l) / prev_l
        if low_diff_pct > 0.01: return False, None

    return True, {
        "tag": "æ‹‰å›ä½ˆå±€",
        "price": round(curr_c, 2),
        "ma5": round(close.rolling(5).mean().iloc[-1], 2),
        "ma10": round(curr_ma10, 2),
        "ma20": round(curr_ma20, 2),
        "ma240": round(curr_ma240, 2)
    }

def check_strategy_vcp_pro(df):
    try:
        close = df['Close']
        volume = df['Volume']

        if len(close) < 260: return False, None

        # ===== 1. è¨ˆç®—æŒ‡æ¨™ =====
        ma10 = close.rolling(10).mean()
        ma20 = close.rolling(20).mean()
        ma50 = close.rolling(50).mean()
        ma150 = close.rolling(150).mean()
        ma200 = close.rolling(200).mean()
        ma60 = close.rolling(60).mean()   # ç¢ºä¿è¨ˆç®— MA60
        ma240 = close.rolling(240).mean() # ç¢ºä¿è¨ˆç®— MA240
        
        # å¸ƒæ—å¸¶ (20æ—¥, 2å€æ¨™æº–å·®)
        std20 = close.rolling(20).std()
        bb_upper = ma20 + (std20 * 2)
        bb_lower = ma20 - (std20 * 2)
        # å¸ƒæ—å¸¶å¯¬åº¦ (Bandwidth)
        bb_width = (bb_upper - bb_lower) / ma20

        # ç•¶å‰æ•¸å€¼
        curr_c = float(close.iloc[-1])
        curr_v = float(volume.iloc[-1]) # ç•¶å¤©æˆäº¤é‡

        curr_ma20 = float(ma20.iloc[-1])
        curr_ma50 = float(ma50.iloc[-1])
        curr_ma150 = float(ma150.iloc[-1])
        curr_ma200 = float(ma200.iloc[-1])
        curr_ma60 = float(ma60.iloc[-1])
        curr_ma240 = float(ma240.iloc[-1])
        curr_bb_width = float(bb_width.iloc[-1])

        # ===== ç¡¬æŒ‡æ¨™éæ¿¾ =====
        # 1. è‚¡åƒ¹å¿…é ˆç«™ä¸Š MA240 (å¹´ç·š)
        if math.isnan(curr_ma240) or curr_c < curr_ma240: return False, None
        
        # 2. [æ–°å¢] è‚¡åƒ¹å¿…é ˆç«™ä¸Š MA60 (å­£ç·š)
        if math.isnan(curr_ma60) or curr_c <= curr_ma60: return False, None
        
        # 3. æˆäº¤é‡ > 500 å¼µ
        if curr_v < 500000: return False, None

        # ===== æ¢ä»¶ 1ï¼šè¶¨å‹¢ç¢ºèª =====
        if curr_c < curr_ma200: return False, None
        if curr_ma200 <= float(ma200.iloc[-20]): return False, None
        if curr_c < curr_ma150: return False, None

        # ===== æ¢ä»¶ 2ï¼šåƒ¹æ ¼ä½éš (é è¿‘ 52 é€±æ–°é«˜) =====
        high_52w = close.iloc[-250:].max()
        low_52w = close.iloc[-250:].min()
        if curr_c < low_52w * 1.3: return False, None
        if curr_c < high_52w * 0.75: return False, None

        # ===== æ¢ä»¶ 3ï¼šæ³¢å‹•æ”¶ç¸® (æ ¸å¿ƒ VCP) =====
        if curr_bb_width > 0.15: return False, None
        if curr_c < curr_ma20 * 0.98: return False, None

        # ===== æ¢ä»¶ 4ï¼šé‡èƒ½éæ¸› =====
        vol_ma5 = volume.rolling(5).mean()
        vol_ma20 = volume.rolling(20).mean()
        if float(vol_ma5.iloc[-1]) > float(vol_ma20.iloc[-1]): return False, None
        if float(vol_ma5.iloc[-1]) < 300000: return False, None

        # ===== æ¢ä»¶ 5 (æ–°å¢)ï¼šå›æª”å¹…åº¦éæ¸› (r1 > r2 > r3) =====
        def calc_retrace(series):
            peak = series.max()
            trough = series.min()
            return (peak - trough) / peak if peak > 0 else 1.0

        r1 = calc_retrace(close.iloc[-60:])
        r2 = calc_retrace(close.iloc[-20:])
        r3 = calc_retrace(close.iloc[-10:])
        
        if not (r1 > r2 > r3): return False, None

    except Exception:
        return False, None

    return True, {
        "tag": "Strict-VCP",
        "price": round(curr_c, 2),
        "ma5": round(close.rolling(5).mean().iloc[-1], 2),
        "ma10": round(ma10.iloc[-1], 2),
        "ma20": round(curr_ma20, 2),
        "ma150": round(curr_ma150, 2),
        "ma200": round(curr_ma200, 2),
        "ma240": round(curr_ma240, 2),
        "bb_width": round(curr_bb_width * 100, 1)
    }

# ==========================================
# 5. æ›´æ–°æ­·å²ç¸¾æ•ˆ (V52 Robust + V53 Clean)
# ==========================================
def update_history_roi(history_db):
    print("===== é–‹å§‹æ›´æ–°æ­·å²ç¸¾æ•ˆ (V53 Clean & Robust) =====")
    tw_tz = pytz.timezone('Asia/Taipei')
    today_str = datetime.now(tw_tz).strftime("%Y/%m/%d")
    today_date = datetime.strptime(today_str, "%Y/%m/%d")

    # 1. åœ¨æ›´æ–°è‚¡åƒ¹å‰ï¼Œå…ˆåŸ·è¡Œã€Œå»é‡ã€
    # é€™æ¨£å¯ä»¥é¿å…å°é‡è¤‡çš„è‚¡ç¥¨å¤šæ¬¡ä¸‹è¼‰è‚¡åƒ¹
    history_db, tracked_ids = remove_duplicates_keep_earliest(history_db)

    if not tracked_ids: 
        print("æ²’æœ‰éœ€è¦è¿½è¹¤çš„è‚¡ç¥¨ã€‚")
        return history_db, tracked_ids

    # é‡æ–°æ”¶é›†å®Œæ•´çš„ symbols (åŒ…å« .TW/.TWO)
    symbols_map = {} # id -> full_symbol
    query_list = set()
    
    for date_str, stocks in history_db.items():
        for stock in stocks:
            symbol = stock['id'] + ('.TW' if stock['type'] == 'ä¸Šå¸‚' else '.TWO')
            symbols_map[stock['id']] = symbol
            query_list.add(symbol)
            
    print(f"è¿½è¹¤è‚¡ç¥¨æ•¸é‡ (ä¸é‡è¤‡): {len(query_list)}")
    current_data = {}
    
    try:
        # ä¸‹è¼‰æœ€è¿‘ 5 å¤©è³‡æ–™
        data = yf.download(list(query_list), period="5d", auto_adjust=True, threads=True)
        
        if data.empty:
            print("âš ï¸ yfinance ä¸‹è¼‰å›å‚³ç‚ºç©ºï¼Œè·³éæ›´æ–°ã€‚")
            return history_db, tracked_ids

        close_df = data['Close'] if 'Close' in data else pd.DataFrame()
        
        # è™•ç†å–®æª”èˆ‡å¤šæª”
        for symbol in query_list:
            try:
                series = None
                if len(query_list) == 1:
                    if isinstance(close_df, pd.DataFrame):
                        if symbol in close_df.columns: series = close_df[symbol]
                        else: series = close_df.iloc[:, 0]
                    else: series = close_df
                else:
                    if symbol in close_df.columns: series = close_df[symbol]
                
                if series is not None and not series.empty:
                    series = series.ffill().dropna()
                    if len(series) >= 2:
                        last_price = float(series.iloc[-1])
                        prev_price = float(series.iloc[-2])
                        # å­˜å…¥ mapï¼Œkey ç”¨ stock_id (å»é™¤ .TW/.TWO) æ–¹ä¾¿å¾ŒçºŒå°æ‡‰
                        stock_id = symbol.split('.')[0]
                        current_data[stock_id] = { 'price': last_price, 'prev': prev_price }
            except: pass

    except Exception as e:
        print(f"âŒ æ­·å²è³‡æ–™ä¸‹è¼‰éŒ¯èª¤: {e}")
        return history_db, tracked_ids

    # æ›´æ–° Database
    updated_count = 0
    for date_str, stocks in history_db.items():
        try:
            entry_date = datetime.strptime(date_str, "%Y/%m/%d")
        except: continue
        
        days_diff = (today_date - entry_date).days

        for stock in stocks:
            s_id = stock['id']
            if s_id in current_data:
                latest_price = current_data[s_id]['price']
                prev_price = current_data[s_id]['prev']
                buy_price = stock['buy_price']
                
                if days_diff <= 0:
                    roi = 0.0
                    daily_change = 0.0
                else:
                    roi = round(((latest_price - buy_price) / buy_price) * 100, 2)
                    daily_change = round(((latest_price - prev_price) / prev_price) * 100, 2)
                
                stock['latest_price'] = round(latest_price, 2)
                stock['roi'] = roi
                stock['daily_change'] = daily_change
                updated_count += 1

    print(f"æ­·å²ç¸¾æ•ˆæ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {updated_count} ç­†è‚¡åƒ¹ã€‚")
    return history_db, tracked_ids

# ==========================================
# 6. ä¸»ç¨‹å¼
# ==========================================
def run_scanner():
    tw_tz = pytz.timezone('Asia/Taipei')
    now = datetime.now(tw_tz)
    
    industry_db = load_json(DB_INDUSTRY)
    history_db = load_json(DB_HISTORY)
    
    # æ­¥é©Ÿ 1: æ›´æ–°æ­·å²ç¸¾æ•ˆ + æ¸…æ´—é‡è¤‡ (å›å‚³ cleaned db å’Œæ‰€æœ‰å·²å­˜åœ¨çš„ ID set)
    history_db, existing_ids = update_history_roi(history_db)
    
    save_json(DB_HISTORY, history_db)
    print("ç›¤ä¸­æ­·å²ç¸¾æ•ˆ (å·²å»é‡) æ›´æ–°è‡³ DBã€‚")

    # é–‹å§‹æƒæä»Šæ—¥æ–°æ¨™çš„
    full_list = get_all_tickers()
    print(f"é–‹å§‹æƒæ... æ™‚é–“: {now.strftime('%H:%M:%S')}")
    
    daily_results = []
    batch_size = 100 
    
    for i in range(0, len(full_list), batch_size):
        batch = full_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{len(full_list)//batch_size + 1}...")
        try:
            data = yf.download(batch, period="2y", group_by='ticker', threads=True, progress=False, auto_adjust=True)
            
            for ticker in batch:
                try:
                    raw_code = ticker.split('.')[0]
                    
                    # ã€V53 é—œéµã€‘å¦‚æœé€™æª”è‚¡ç¥¨å·²ç¶“åœ¨æ­·å²åå–®ä¸­ï¼Œç›´æ¥è·³éè¨ˆç®—ï¼Œç¯€çœæ™‚é–“ä¸¦é¿å…é‡è¤‡
                    if raw_code in existing_ids:
                        continue

                    df = pd.DataFrame()
                    if len(batch) > 1:
                        if ticker in data.columns.levels[0]:
                            df = data[ticker].copy()
                    else:
                        df = data.copy()
                    
                    df = df.dropna()
                    if df.empty: continue
                    if isinstance(df.columns, pd.MultiIndex):
                        df.columns = df.columns.droplevel(0)

                    required_cols = ['Close', 'Volume', 'Low']
                    if not all(col in df.columns for col in required_cols): continue

                    is_match_1, info_1 = check_strategy_original(df)
                    is_match_2, info_2 = check_strategy_vcp_pro(df)
                    
                    final_match = False
                    final_info = {}
                    strategy_tags = []

                    if is_match_1:
                        final_match = True
                        final_info = info_1
                        strategy_tags.append("æ‹‰å›ä½ˆå±€")
                    if is_match_2:
                        final_match = True
                        if not final_info: final_info = info_2
                        strategy_tags.append("Strict-VCP")
                    
                    if final_match:
                        name = raw_code
                        if raw_code in twstock.codes: name = twstock.codes[raw_code].name
                        group = get_stock_group(raw_code, industry_db)
                        if raw_code not in industry_db: industry_db[raw_code] = group
                        
                        try:
                            prev_c = df['Close'].iloc[-2]
                            change_rate = round((final_info['price'] - prev_c) / prev_c * 100, 2)
                        except:
                            change_rate = 0.0
                            
                        tags_str = " & ".join(strategy_tags)
                        note_ma240 = round(final_info.get('ma240', 0), 2)
                        note_str = f"{tags_str} / å¹´ç·š{note_ma240}"

                        stock_entry = {
                            "id": raw_code,
                            "name": name,
                            "group": group,
                            "type": "ä¸Šæ«ƒ" if ".TWO" in ticker else "ä¸Šå¸‚",
                            "price": final_info['price'], 
                            "ma5": final_info['ma5'],
                            "ma10": final_info['ma10'],
                            "changeRate": change_rate,
                            "isValid": True,
                            "note": note_str,
                            "buy_price": final_info['price'], 
                            "latest_price": final_info['price'], 
                            "roi": 0.0, 
                            "daily_change": change_rate
                        }
                        
                        daily_results.append(stock_entry)
                        print(f" -> Found: {raw_code} {name} [{tags_str}]")
                        
                except Exception: continue
        except Exception as e:
            print(f"Batch error: {e}")
            continue
        
        time.sleep(1.5)

    save_json(DB_INDUSTRY, industry_db)
    
    # è™•ç† output
    print(f"æƒæçµæŸï¼Œå…±ç™¼ç¾ {len(daily_results)} æª”ã€‚æ›´æ–° data.json...")
    data_payload = {
        "date": now.strftime("%Y/%m/%d %H:%M:%S"),
        "source": "GitHub Actions",
        "list": daily_results
    }
    save_json(DATA_JSON, data_payload)

    # æ­¸æª”åˆ¤å®š
    current_time = now.time()
    market_open = dt_time(9, 0, 0)
    market_close = dt_time(13, 30, 0)
    is_market_session = market_open <= current_time <= market_close

    if is_market_session:
        print(f"âš ï¸ ç¾åœ¨æ˜¯ç›¤ä¸­æ™‚é–“ ({current_time.strftime('%H:%M')})ï¼Œè·³é History æ–°å¢æ­¸æª” (ä½†å·²æ›´æ–°èˆŠè‚¡åƒ¹)ã€‚")
    else:
        if current_time > market_close:
            record_date_str = now.strftime("%Y/%m/%d")
        else:
            yesterday = now - timedelta(days=1)
            record_date_str = yesterday.strftime("%Y/%m/%d")

        print(f"âœ… ç›¤å¾Œæ™‚æ®µï¼Œæº–å‚™å°‡æ–°è³‡æ–™æ­¸æª”è‡³ History: {record_date_str}")
        
        if daily_results:
            # å†æ¬¡æª¢æŸ¥ï¼šå¦‚æœé€™äº›çµæœå·²ç¶“åœ¨ DB è£¡äº† (ä»¥é˜²è¬ä¸€)ï¼Œå°±ä¸é‡è¤‡åŠ 
            if record_date_str not in history_db:
                history_db[record_date_str] = daily_results
            else:
                # åˆä½µé‚è¼¯: ç¢ºä¿ä¸é‡è¤‡
                existing_today_ids = {s['id'] for s in history_db[record_date_str]}
                for res in daily_results:
                    if res['id'] not in existing_today_ids and res['id'] not in existing_ids:
                         history_db[record_date_str].append(res)

            sorted_history = dict(sorted(history_db.items(), reverse=True))
            save_json(DB_HISTORY, sorted_history)
            print("History.json æ–°å¢å®Œç•¢ã€‚")
        else:
            print("ä»Šæ—¥ç„¡ç¬¦åˆç­–ç•¥æ¨™çš„ï¼Œä¸æ–°å¢ Historyã€‚")

    return daily_results

if __name__ == "__main__":
    run_scanner()
